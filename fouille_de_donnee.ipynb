{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f5e4d7",
   "metadata": {},
   "source": [
    "1.1 Code - Chargement et Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31a7ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "print(pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7246c8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\UNiK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\UNiK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                category                                                log  \\\n",
      "0  authentication-failed  [Tue Apr 11 14:36:11 2000] [error] [client 28....   \n",
      "1  authentication-failed  [Tue Jan 21 17:01:07 2020] [error] [client 108...   \n",
      "2  authentication-failed  [Thu Oct 12 01:17:44 2023] [ malfunction error...   \n",
      "3  authentication-failed  [Tue Jul 30 16:18:08 2013] [error] [client 217...   \n",
      "4  authentication-failed  [Thu Sep 10 05:09:58 2015] [error] [client 2.5...   \n",
      "\n",
      "                                           clean_log  \n",
      "0  user jessicakaiser authentication failure here...  \n",
      "1  user mejianathan authentication failure powerg...  \n",
      "2  user anthony authentication failure beathoweve...  \n",
      "3  user bushcassandra authentication failure agre...  \n",
      "4  user dylanlewis authentication failure expectt...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 1) Charger les données\n",
    "# df = pd.read_csv(r\"C:\\Users\\UNiK\\Desktop\\output_0.1.log\", header=None, names=[\"log\"], sep=\"\\n\")\n",
    "\n",
    "with open(r\"C:\\Users\\UNiK\\Desktop\\output_0.1.log\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Exemple : extraction manuelle des catégories et logs\n",
    "categories = []\n",
    "logs = []\n",
    "\n",
    "for line in lines[1:]:  # si la première ligne est header\n",
    "    # Supposons que la catégorie est tout avant la première virgule\n",
    "    parts = line.split(\",\", 1)\n",
    "    if len(parts) == 2:\n",
    "        categories.append(parts[0].strip())\n",
    "        logs.append(parts[1].strip())\n",
    "    else:\n",
    "        categories.append(\"unknown\")\n",
    "        logs.append(line.strip())\n",
    "\n",
    "df = pd.DataFrame({\"category\": categories, \"log\": logs})\n",
    "\n",
    "# 2) Nettoyage des logs\n",
    "def nettoyer_texte(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)        # Retirer les crochets et leur contenu\n",
    "    text = re.sub(r'\\d+', '', text)            # Retirer les chiffres (dates, ports)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)        # Retirer ponctuation\n",
    "    text = re.sub(r'http\\S+', '', text)        # Retirer URLs\n",
    "    return text\n",
    "\n",
    "df['clean_log'] = df['log'].apply(nettoyer_texte)\n",
    "\n",
    "# 3) Tokenisation, Stopwords et Lemmatisation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pretraitement(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['clean_log'] = df['clean_log'].apply(pretraitement)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a9229e",
   "metadata": {},
   "source": [
    "1.2 Vectorisation TF-IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a592bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division en Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['clean_log'], df['category'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf36b36",
   "metadata": {},
   "source": [
    "2. Modélisation & Surapprentissage\n",
    "2.1 Sur-apprentissage volontaire\n",
    "On va entraîner un petit réseau de neurones MLPClassifier avec beaucoup d’itérations et peu de régularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b834e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SURAPPRENTISSAGE ===\n",
      "Train Accuracy : 0.95975\n",
      "Test Accuracy  : 0.9174285714285715\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "       authentication-failed       0.93      0.91      0.92       222\n",
      "      authentication-success       0.84      0.86      0.85       236\n",
      "           connection-closed       0.89      0.91      0.90       253\n",
      "           connection-failed       0.96      0.95      0.95       233\n",
      "           connection-opened       0.92      0.89      0.90       241\n",
      "          database-operation       1.00      0.98      0.99       221\n",
      "           directory-changed       0.88      0.68      0.76       231\n",
      "           directory-created       0.99      0.98      0.98       243\n",
      "           directory-deleted       0.80      0.95      0.87       236\n",
      "         file-action-failure       1.00      0.99      1.00       217\n",
      "                file-deleted       0.98      0.77      0.87       248\n",
      "           file-modification       0.78      0.97      0.86       222\n",
      "                   file-read       1.00      1.00      1.00       241\n",
      "                  file-write       1.00      1.00      1.00       230\n",
      "         hardware-monitoring       1.00      0.99      0.99       217\n",
      "        http-request-failure       0.98      0.96      0.97       236\n",
      "        http-request-success       0.99      0.99      0.99       235\n",
      "                   ids-alert       1.00      0.99      1.00       244\n",
      "            network-filtered       0.96      0.88      0.92       252\n",
      "             network-traffic       0.99      1.00      1.00       238\n",
      "               process-ended       0.86      0.74      0.80       247\n",
      "               process-error       1.00      0.99      0.99       215\n",
      "                process-info       1.00      0.99      0.99       251\n",
      "            process-shutdown       0.85      0.63      0.72       212\n",
      "             process-started       0.94      0.89      0.91       250\n",
      "system-configuration-changed       0.97      0.87      0.92       215\n",
      "               user-creation       0.56      0.95      0.71       251\n",
      "               user-deletion       0.98      0.99      0.98       222\n",
      "                 user-logout       0.88      0.84      0.86       218\n",
      "           user-session-open       0.96      0.97      0.97       223\n",
      "\n",
      "                    accuracy                           0.92      7000\n",
      "                   macro avg       0.93      0.92      0.92      7000\n",
      "                weighted avg       0.93      0.92      0.92      7000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Sur-apprentissage forcé (trop de neurones, beaucoup d'epochs)\n",
    "model_overfit = MLPClassifier(hidden_layer_sizes=(100,100,100),\n",
    "                              max_iter=1000,\n",
    "                              alpha=0.00001,  # quasi pas de régularisation\n",
    "                              random_state=42)\n",
    "model_overfit.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred_train = model_overfit.predict(X_train_vec)\n",
    "y_pred_test = model_overfit.predict(X_test_vec)\n",
    "\n",
    "print(\"=== SURAPPRENTISSAGE ===\")\n",
    "print(\"Train Accuracy :\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Test Accuracy  :\", accuracy_score(y_test, y_pred_test))\n",
    "print(classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c4f5f",
   "metadata": {},
   "source": [
    "3. Correction\n",
    "Méthode 1 : Régularisation + Dropout simulé\n",
    "On augmente alpha (régularisation L2) et réduit la complexité du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b6ea9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CORRECTION MÉTHODE 1 ===\n",
      "Test Accuracy : 0.925\n"
     ]
    }
   ],
   "source": [
    "model_reg = MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                          max_iter=300,\n",
    "                          alpha=0.01,\n",
    "                          random_state=42)\n",
    "model_reg.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred_test_reg = model_reg.predict(X_test_vec)\n",
    "print(\"=== CORRECTION MÉTHODE 1 ===\")\n",
    "print(\"Test Accuracy :\", accuracy_score(y_test, y_pred_test_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45922f",
   "metadata": {},
   "source": [
    "Méthode 2 : Validation croisée + Optimisation Hyperparamètres (GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d62259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UNiK\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:788: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,)],\n",
    "    'alpha': [0.001, 0.01, 0.1],\n",
    "    'max_iter': [200, 300]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(MLPClassifier(random_state=42),\n",
    "                    param_grid, cv=3, scoring='accuracy')\n",
    "grid.fit(X_train_vec, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_vec)\n",
    "print(\"=== CORRECTION MÉTHODE 2 ===\")\n",
    "print(\"Meilleurs paramètres :\", grid.best_params_)\n",
    "print(\"Test Accuracy :\", accuracy_score(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eda62bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\UNiK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\UNiK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive Bayes ===\n",
      "Test Accuracy : 0.8985714285714286\n",
      "\n",
      "=== Logistic Regression ===\n",
      "Meilleurs paramètres : {'C': 10}\n",
      "Test Accuracy : 0.9264285714285714\n",
      "\n",
      "=== MLPClassifier ===\n",
      "Meilleurs paramètres : {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'max_iter': 200}\n",
      "Test Accuracy : 0.9271428571428572\n",
      "\n",
      "Meilleur modèle : MLPClassifier\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "       authentication-failed       0.93      0.94      0.93       222\n",
      "      authentication-success       0.90      0.88      0.89       236\n",
      "           connection-closed       0.90      0.95      0.92       253\n",
      "           connection-failed       0.96      0.94      0.95       233\n",
      "           connection-opened       0.93      0.92      0.92       241\n",
      "          database-operation       1.00      1.00      1.00       221\n",
      "           directory-changed       0.96      0.69      0.80       231\n",
      "           directory-created       1.00      0.99      0.99       243\n",
      "           directory-deleted       0.80      0.96      0.87       236\n",
      "         file-action-failure       1.00      1.00      1.00       217\n",
      "                file-deleted       0.99      0.77      0.87       248\n",
      "           file-modification       0.79      1.00      0.88       222\n",
      "                   file-read       0.99      1.00      1.00       241\n",
      "                  file-write       1.00      1.00      1.00       230\n",
      "         hardware-monitoring       1.00      1.00      1.00       217\n",
      "        http-request-failure       0.99      0.97      0.98       236\n",
      "        http-request-success       1.00      0.99      1.00       235\n",
      "                   ids-alert       1.00      1.00      1.00       244\n",
      "            network-filtered       0.98      0.87      0.92       252\n",
      "             network-traffic       1.00      1.00      1.00       238\n",
      "               process-ended       0.92      0.75      0.82       247\n",
      "               process-error       1.00      0.99      1.00       215\n",
      "                process-info       1.00      1.00      1.00       251\n",
      "            process-shutdown       0.82      0.64      0.72       212\n",
      "             process-started       0.98      0.90      0.94       250\n",
      "system-configuration-changed       0.98      0.88      0.93       215\n",
      "               user-creation       0.56      0.98      0.71       251\n",
      "               user-deletion       0.98      0.99      0.98       222\n",
      "                 user-logout       0.92      0.86      0.89       218\n",
      "           user-session-open       0.96      0.97      0.97       223\n",
      "\n",
      "                    accuracy                           0.93      7000\n",
      "                   macro avg       0.94      0.93      0.93      7000\n",
      "                weighted avg       0.94      0.93      0.93      7000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ----------- Prétraitement identique -----------------\n",
    "\n",
    "def nettoyer_texte(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)        # Retirer crochets\n",
    "    text = re.sub(r'\\d+', '', text)            # Retirer chiffres\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)        # Retirer ponctuation\n",
    "    text = re.sub(r'http\\S+', '', text)        # Retirer URLs\n",
    "    return text\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pretraitement(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Charger fichier brut, extraire catégories et logs\n",
    "with open(r\"C:\\Users\\UNiK\\Desktop\\output_0.1.log\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "categories = []\n",
    "logs = []\n",
    "for line in lines[1:]:\n",
    "    parts = line.split(\",\", 1)\n",
    "    if len(parts) == 2:\n",
    "        categories.append(parts[0].strip())\n",
    "        logs.append(parts[1].strip())\n",
    "    else:\n",
    "        categories.append(\"unknown\")\n",
    "        logs.append(line.strip())\n",
    "\n",
    "df = pd.DataFrame({\"category\": categories, \"log\": logs})\n",
    "df['clean_log'] = df['log'].apply(nettoyer_texte).apply(pretraitement)\n",
    "\n",
    "# Vectorisation TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=2000, min_df=2, max_df=0.8)\n",
    "X = vectorizer.fit_transform(df['clean_log'])\n",
    "y = df['category']\n",
    "\n",
    "# Train/test split\n",
    "X_train_vec, X_test_vec, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ----------- Modèle 1 : Naive Bayes ---------------\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train_vec, y_train)\n",
    "y_pred_nb = model_nb.predict(X_test_vec)\n",
    "print(\"=== Naive Bayes ===\")\n",
    "print(\"Test Accuracy :\", accuracy_score(y_test, y_pred_nb))\n",
    "\n",
    "# ----------- Modèle 2 : Logistic Regression + GridSearch ---------------\n",
    "param_grid_lr = {'C': [0.1, 1, 10]}\n",
    "grid_lr = GridSearchCV(LogisticRegression(max_iter=200), param_grid_lr, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_lr.fit(X_train_vec, y_train)\n",
    "best_lr = grid_lr.best_estimator_\n",
    "y_pred_lr = best_lr.predict(X_test_vec)\n",
    "print(\"\\n=== Logistic Regression ===\")\n",
    "print(\"Meilleurs paramètres :\", grid_lr.best_params_)\n",
    "print(\"Test Accuracy :\", accuracy_score(y_test, y_pred_lr))\n",
    "\n",
    "# ----------- Modèle 3 : MLPClassifier + GridSearch rapide ---------------\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,)],\n",
    "    'alpha': [0.01],\n",
    "    'max_iter': [200]\n",
    "}\n",
    "grid_mlp = GridSearchCV(MLPClassifier(random_state=42), param_grid_mlp, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_mlp.fit(X_train_vec, y_train)\n",
    "best_mlp = grid_mlp.best_estimator_\n",
    "y_pred_mlp = best_mlp.predict(X_test_vec)\n",
    "print(\"\\n=== MLPClassifier ===\")\n",
    "print(\"Meilleurs paramètres :\", grid_mlp.best_params_)\n",
    "print(\"Test Accuracy :\", accuracy_score(y_test, y_pred_mlp))\n",
    "\n",
    "# ----------- Rapport détaillé pour le meilleur modèle ---------------\n",
    "best_score = max(\n",
    "    accuracy_score(y_test, y_pred_nb),\n",
    "    accuracy_score(y_test, y_pred_lr),\n",
    "    accuracy_score(y_test, y_pred_mlp)\n",
    ")\n",
    "\n",
    "if best_score == accuracy_score(y_test, y_pred_nb):\n",
    "    print(\"\\nMeilleur modèle : Naive Bayes\")\n",
    "    print(classification_report(y_test, y_pred_nb))\n",
    "elif best_score == accuracy_score(y_test, y_pred_lr):\n",
    "    print(\"\\nMeilleur modèle : Logistic Regression\")\n",
    "    print(classification_report(y_test, y_pred_lr))\n",
    "else:\n",
    "    print(\"\\nMeilleur modèle : MLPClassifier\")\n",
    "    print(classification_report(y_test, y_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89ec1b",
   "metadata": {},
   "source": [
    "5. Déploiement Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e0589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title(\"Détection d'échec d'authentification\")\n",
    "text_input = st.text_area(\"Entrez un log à analyser\")\n",
    "\n",
    "if st.button(\"Prédire\"):\n",
    "    text_clean = pretraitement(nettoyer_texte(text_input))\n",
    "    vec = vectorizer.transform([text_clean])\n",
    "    pred = best_model.predict(vec)[0]\n",
    "    st.write(\"### Prédiction :\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cccbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Supposons que ton modèle final s'appelle best_model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m joblib.dump(\u001b[43mbest_model\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mbest_model.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Et ton vectorizer aussi\u001b[39;00m\n\u001b[32m      7\u001b[39m joblib.dump(vectorizer, \u001b[33m\"\u001b[39m\u001b[33mvectorizer.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "joblib.dump(best_model, \"best_model.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "print(\"✅ Modèle et vectorizer sauvegardés !\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
